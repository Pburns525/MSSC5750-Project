---
title: "Final Project"
author: "David, Michael, Patrick"
format: html
---


```{r echo = FALSE, message=FALSE}
library(MASS)
library(ggplot2)
library(dplyr)
library(scales)
library(glmnet)
library(fastDummies)
library(caret)
library(shapr)
library(shapviz)
library(party)
library(reshape2)
library(gridExtra)

# Importing data set from github repository
data <- read.csv("https://raw.githubusercontent.com/Pburns525/MSSC5750-Project/refs/heads/main/loan_data.csv")

```


High-Level Data Exploration
```{r}

nrow(data)
colnames(data)

# There is no missing data, already a cleaned data set
colSums(is.na(data))

table(data$person_gender)

# Education is not currently ordered, change to a factor and set levels
data$person_education<- factor(data$person_education, levels=c('High School','Associate','Bachelor','Master','Doctorate'), ordered=TRUE)
table(data$person_education)

# Looks like we have somebody who is 144 yrs old, 7 who are over age 100. PRobably want to remove them, its doubtful anybody very old would be applying anyways, and these ages are not really possible
summary(data$person_age)
sum(data$person_age>100)

### quick summarizations to give us an idea of what we need to visualize/look closer at
summary(data$person_income)

summary(data$loan_amnt)

table(data$loan_intent)

summary(data$cb_person_cred_hist_length)

summary(data$loan_percent_income)

summary(data$credit_score)

table(data$person_home_ownership)

# This is years of employment experience. Again we see some super high values, longer than human lifespan, so should clean those up
summary(data$person_emp_exp)
sum(data$person_emp_exp>70)

# Hm. In every single case where there was a prior loan default, loan_status=0. We probably don't want to include this variable in anything explanatory
table(data$previous_loan_defaults_on_file, data$loan_status)

# 22% of our data receives a loan, so our dataset is imbalanced, but not too extreme
percent(table(data$loan_status)[2]/nrow(data),accuracy=1)

# Oddly higher credit score does not seem to make a difference
cor(data$credit_score,data$loan_status)
summary(data$credit_score)
lowcred<-data %>% filter(credit_score<601)
goodcred<-data %>% filter(credit_score>670)
mean(lowcred$loan_status)
mean(goodcred$loan_status)

# Looking at age/credit score with loan approval. NO clear patterns
ggplot(data=rbind( data %>% filter(loan_status==1) %>%  sample_n(1000), data %>% filter(loan_status==0) %>%  sample_n(1000)), aes(x=credit_score, y=person_age, fill=loan_status, color=loan_status)) + 
  geom_point() + ylim(20,60) + xlim(400,780)


```  

Creating some basic visualizations
```{R}

# Removing outliers (Somebody was earning like 7M, we have almost 25 who are over 1M still)
ggplot(data=data %>% filter(person_income<500000), aes(x=person_income)) +geom_histogram(bins=30)


#Actually might be better to just add a catch bucket at the end. IF over 500k, just set to 500k
inc1<-ggplot(data=data %>% mutate(person_income2 = if_else(person_income<500000,person_income,500000)), aes(x=person_income2)) +geom_histogram(bins=40, fill='steelblue',color='black') + xlab("Income") + ggtitle("Histogram of Income") +
  scale_x_continuous(breaks=seq(0,500000,50000), labels=c("$0","$50,000","$100,000","$150,000","$200,000","$250,000","$300,000","$350,000","$400,000","$450,000", "$500,000+") )

# Visual for log adjusted income
inc2<-ggplot(data = data %>% mutate('log(person_income)' = log(ifelse(person_income>500000,500000,person_income))  ), aes(x=log(person_income))) +geom_histogram(bins=40, fill='orange',color='black') + xlab("Log of Income") + ggtitle("Histogram of Log of Income") 

grid.arrange(inc1, inc2, nrow = 2)

# Loan intent
temp<-data %>% summarize('0'=1-mean(loan_status),"1"=mean(loan_status) ,.by=loan_intent)
intent<-melt(temp) %>% dplyr::rename(loan_status=variable)

ggplot(data=intent %>% filter(loan_status==1), aes(x=loan_intent, y=value)) + 
  geom_col(position='stack', fill="violetred2") + 
  geom_text(aes(label = percent(value,accuracy=1)), 
            position = position_stack(vjust = 0.5), 
            size = 4) + ggtitle('Loan Repayment by Intent') + 
  scale_y_continuous(name='', labels = scales::percent, limits=c(0,.4)) + xlab('') +
  theme(panel.grid.minor.y=element_blank())

# Education
temp<-data %>% summarize('0'=1-mean(loan_status),"1"=mean(loan_status) ,.by=person_education)
educ<-melt(temp) %>% dplyr::rename(loan_status=variable)

ggplot(data=educ %>% filter(loan_status==1), aes(x=person_education, y=value)) +
    geom_col(position='stack', fill='aquamarine3') +
    geom_text(aes(label = percent(value,accuracy=1)), 
            position = position_stack(vjust = 0.5), 
            size = 4) + ggtitle('Loan Repayment by Education') + 
  scale_y_continuous(name='', labels = scales::percent, limits=c(0,.4)) + xlab('') +
  theme(panel.grid.minor.y=element_blank())

### cred hist length
ggplot(data=data %>% summarize(count=n(),.by=cb_person_cred_hist_length), aes(x= cb_person_cred_hist_length, y=count)) + geom_col(fill='steelblue') + ggtitle('Frequencies for credit history length') + scale_x_continuous(breaks=c(2,seq(10,30,5)) )

```


Data cleaning
```{R}

data2<-data
######## Transformation of variables ########

#### person_income ####

# Salary is extremely right skewed, lets use the log transformation. Minimum is 8000 so we do not need to add a constant
min(data2$person_income)
data2$log_person_income <- log(data2$person_income)

# Our max is still way larger than our 3rd quartile, so maybe we should cap salary before transformation
summary(data2$log_person_income)
data2$log_person_income<-log(ifelse(data2$person_income>500000,500000,data2$person_income))

# Better
summary(data2$log_person_income)


#### person_emp_exp ####

# Employee experience also very skewed. And some of the values dont really make any sense. Log transformation can fix this too
summary(data2$person_emp_exp)

# First replace extreme (impossible) values with something realistic
data2$person_emp_exp<-ifelse(data2$person_emp_exp>50,50,data2$person_emp_exp)

data2$log_person_emp_exp <- log(data2$person_emp_exp+.5)

# Better
summary(data2$log_person_emp_exp)


#### Age  ####
# We need to bucket our age. Theres no way we actually have someone aged 144, and 75% of our data falls between 20-30
data2<- data2 %>% mutate(person_age = case_when(person_age<25 ~ "<25",
                                                person_age<=30 ~"25-30",
                                                person_age<45 ~ "31-44",
                                                person_age<60 ~ "45-59",
                                                TRUE ~ "60+"))

data2$person_age<-as.factor(data2$person_age)
table(data2$person_age)

####  cred hist length  ####
# Only a handful fall >=18, we will just cap it all there as a "catch all", but leave as numeric
table(data2$cb_person_cred_hist_length)
data2$cb_person_cred_hist_length <- ifelse(data2$cb_person_cred_hist_length>18,18, data2$cb_person_cred_hist_length)



######## One hot encoding for categorical variables ########

# Preparing categorical variables.
data2$person_gender <- as.factor(data2$person_gender)
data2$person_home_ownership<-as.factor(data2$person_home_ownership)
data2$loan_intent<-as.factor(data2$loan_intent)
data2$previous_loan_defaults_on_file<-as.factor(data2$previous_loan_defaults_on_file)

# One hot encoding for categorical variables
data3 <- dummy_cols(data2, remove_first_dummy = FALSE, remove_selected_columns = TRUE)

# we should remove first dummy to prevent multicolinnearity , need to keep track of what was dropped because that is our reference value
data3<-data3 %>% select(-c("person_gender_female","person_education_High School","person_home_ownership_MORTGAGE", "loan_intent_DEBTCONSOLIDATION", "previous_loan_defaults_on_file_No", "person_age_<25"))




# replacing base data with cleaner version now
cleanData <- data3

```


 Outline for LASSO regression to help with variable selection
```{R}


# Extracting outcome variable
class <- cleanData$loan_status

# Converting to a matrix, exclude the non-log-transformed, and also the prior loan default
matrixData <- model.matrix(class ~ ., data = cleanData %>% select(-c("loan_status", "person_income", "person_emp_exp", "previous_loan_defaults_on_file_Yes") ))

# First column is all intercepts
matrixData<- matrixData[,-1]

# Uses cross validation, no need to train/test split. Alpha selects lasso (rather than ridge regression), but I think thats the default anyways
lasso <- cv.glmnet(matrixData, class, alpha = 1, family = "binomial")

# See coefficients, which variables we might drop
# Now that we switched to log income, variable is no longer being dropped. We will proceed with log_income
coef(lasso)

# Make predictions
cleanData$lassoPred <- as.vector(predict(lasso, newx = matrixData, type = "class"))

# Evaluate accuracy measures
confusionMatrix(factor(cleanData$lassoPred), factor(cleanData$loan_status), positive = '1')

# And again with our loan_default knowledge:
confusionMatrix(factor( ifelse(cleanData$previous_loan_defaults_on_file_Yes==1,0,cleanData$lassoPred)), factor(cleanData$loan_status), positive = '1')


```

Now doing a basic logistic regression with/without lasso selection
```{R}

# We need a train/test split. We will use 80/20
# can use uniform distribution, set seed for reproducability
set.seed(100)
forTrain<-runif(nrow(cleanData))<.8

trainData<-cleanData[forTrain,]
testData<-cleanData[!forTrain,]

# First creating logistic regression with all our variables
logModel <- glm(loan_status ~ . , data=trainData %>% select(-c("lassoPred", "person_income", "person_emp_exp", "previous_loan_defaults_on_file_Yes")), family= binomial )

# Repeating but only with variables selected by lasso
logModelLasso <- glm(loan_status ~ loan_int_rate + loan_percent_income + log_person_income + person_home_ownership_OTHER + person_home_ownership_OWN + person_home_ownership_RENT + loan_intent_EDUCATION + loan_intent_HOMEIMPROVEMENT + loan_intent_PERSONAL + loan_intent_VENTURE, data=trainData, family= binomial )

testData$logisticPred <- round(predict(logModel, testData, type='response'))
testData$logisticLassoPred <- round(predict(logModelLasso, testData, type='response'))

# Matrix for regular log
confusionMatrix(factor(testData$logisticPred), factor(testData$loan_status), positive = '1')

# Matrix for lasso log
confusionMatrix(factor(testData$logisticLassoPred), factor(testData$loan_status), positive = '1')


# Our training data is still heavily imbalanced, repeating again for rebalanced data
table(trainData$loan_status)
ones<-trainData %>% filter(loan_status==1)
set.seed(5)
zeros<-trainData %>% filter(loan_status==0) %>% sample_n(8000)
trainDataBalanced<-rbind(ones,zeros)
table(trainDataBalanced$loan_status)

rebalancedLog <- glm(loan_status ~ . , data=trainDataBalanced %>% select(-c("lassoPred", "person_income", "person_emp_exp", "previous_loan_defaults_on_file_Yes")), family= binomial )

testData$rebalancedLog <- round(predict(rebalancedLog, testData, type='response'))

# Matrix for regular log
confusionMatrix(factor(testData$rebalancedLog), factor(testData$loan_status), positive = '1')


```


Creating dataframe of results
```{R}

results<-data.frame(model=character(),accuracy=numeric(),sensitivity=numeric(),specificity=numeric())

addresults<-function(name,res){
  results<<-rbind(results, data.frame(model=name,accuracy=res$overall['Accuracy'],sensitivity=res$byClass['Sensitivity'],specificity=res$byClass['Specificity']) )
}

addresults('Lasso', confusionMatrix(factor( cleanData$lassoPred), factor(cleanData$loan_status), positive = '1'))
addresults('Log', confusionMatrix(factor(testData$logisticPred), factor(testData$loan_status), positive = '1'))
addresults('LogLasso', confusionMatrix(factor(testData$logisticLassoPred), factor(testData$loan_status), positive = '1'))
addresults('LogBalanced', confusionMatrix(factor(testData$rebalancedLog), factor(testData$loan_status), positive = '1'))
results

###################### Repeating for accuracy after applying logic for previous default ##############################

results2<-data.frame(model=character(),accuracy=numeric(),sensitivity=numeric(),specificity=numeric())

addresults2<-function(name,res){
  results2<<-rbind(results2, data.frame(model=name,accuracy=res$overall['Accuracy'],sensitivity=res$byClass['Sensitivity'],specificity=res$byClass['Specificity']) )
}

addresults2('Lasso', confusionMatrix(factor(ifelse(cleanData$previous_loan_defaults_on_file_Yes==1,0,cleanData$lassoPred) ), factor(cleanData$loan_status), positive = '1'))
addresults2('Log', confusionMatrix(factor( ifelse(testData$previous_loan_defaults_on_file_Yes==1,0, testData$logisticPred) ), factor(testData$loan_status), positive = '1'))
addresults2('LogLasso', confusionMatrix(factor(ifelse(testData$previous_loan_defaults_on_file_Yes==1,0,testData$logisticLassoPred) ), factor(testData$loan_status), positive = '1'))
addresults2('LogBalanced', confusionMatrix(factor(ifelse(testData$previous_loan_defaults_on_file_Yes==1,0,testData$rebalancedLog) ), factor(testData$loan_status), positive = '1'))
results2

```




Shap values/viz
```{r}
# Requires split out xtrain, ytrain, xtest.

xtrain<-trainDataBalanced %>% select(-c("lassoPred", "person_income", "person_emp_exp", "previous_loan_defaults_on_file_Yes", "loan_status"))
ytrain<-trainDataBalanced %>% select(loan_status)

xtest<-testData  %>% select(-c("lassoPred", "person_income", "person_emp_exp", "previous_loan_defaults_on_file_Yes", "loan_status", "logisticPred","logisticLassoPred", "rebalancedLog"))

# calculating shap is costly so we will reduce the size of the data we pass in

# we will just explain on our train data as well for larger sample size
set.seed(5)
xtestSmall <- xtest[sample(nrow(xtest), 250), ]
explainer<- shapr::explain(model=rebalancedLog, x_train=xtrain, x_explain=xtestSmall, approach="ctree", phi0=mean(ytrain$loan_status))

shp <- shapviz(explainer)
sv_importance(shp)

plot(explainer, plot_phi0 = FALSE, index_x_explain = 1)
#### 

###
shapObj <-shapviz(explainer)
sv_importance(shapObj, kind="beeswarm", show_numbers = TRUE, max_display=25)
sv_importance(shapObj, kind="beeswarm", show_numbers = TRUE, max_display=10)

```

## ADD CODE FOR LDA

```{r}
# Originally was using prior loan default as outcome variable
# Switched to loan_status to match the target variable we used for everything else

library(tidyverse)
library(MASS)
loan_data <- read.csv("https://raw.githubusercontent.com/Pburns525/MSSC5750-Project/refs/heads/main/loan_data.csv")
bankr<-data.frame(loan_data$person_age,loan_data$person_income,loan_data$person_emp_exp,loan_data$loan_amnt,
                  loan_data$loan_int_rate,loan_data$loan_percent_income,loan_data$cb_person_cred_hist_length,
                  loan_data$credit_score,loan_data$loan_status)

head(bankr)



lda_desc<-lda(loan_data$loan_status~loan_data$person_age+
                loan_data$person_income + loan_data$person_emp_exp+loan_data$loan_amnt +
                loan_data$loan_int_rate+ loan_data$loan_percent_income+loan_data$cb_person_cred_hist_length+
                loan_data$credit_score,data=bankr )


lda_desc

a<-lda_desc$scaling
a


library(MVN)
library(tidyverse)

# renaming previous_default to loan_approved
# previous name previous_default
loan_repaid<-bankr %>%
  filter(loan_data.loan_status==1) %>%
  select(-loan_data.loan_status)
  s1<-cov(loan_repaid)
  
  s1
  
  

loan_repaid
# Checking for multivariate normality
repaycheck<-mvn(loan_repaid[,1:8],mvnTest = "mardia")$multivariateNormality
print(repaycheck)

#separate groups into two and compute sample covariance matrix
# previous name no_default
loan_default<-bankr%>%
  filter(loan_data.loan_status ==0)%>%
  select(-loan_data.loan_status)
s2<-cov(loan_default)
s2

# pooled sample covariance matrix  is :

n1<-nrow(loan_repaid)

n2<-nrow(loan_default)

Spl<-1/(n1+n2-2)*((n1-1)*s1 +(n2-1)*s2)

Spl

# manually calculating the coefficients of the standardized discriminant function

a.star<-sqrt(diag(Spl))*a

a.star
# a.star ranks the independependent variables in the order of discriminative capacity
# i.e which variables best separate the groups: load_default on file vs no history of default

# 1 age
#2. income
#3. employment experience
#4. loan amount
#5. loan interest rate
# 6. loan percent of income (i.e debt-to-income ratio)
#7.  length of credit history
# 8. Credit score

library(tidyverse)
ggplot(bankr,aes(x=loan_data$person_age,y=loan_data$loan_percent_income,
                 color=factor(loan_data$loan_status)))+
  geom_point()+ labs(xlab="Age",ylab = "debt-to-incomes")+
  scale_color_discrete(name="loan_data$loan_status")



repaidCheck<-mvn(loan_repaid[,1:8],mvnTest = "mardia")$multivariateNormality

print(repaidCheck)

pca_bank<-prcomp(bankr[,1:8],scale. = TRUE)
summary(pca_bank)

pca_bank # pca to confirm the variability in data set and ranking of variables 
           # in discrimination  b/w 2 groups


lda2<-lda(loan_data.loan_status~loan_data.person_age + loan_data.person_income+
            loan_data.person_emp_exp +loan_data.loan_amnt,data =bankr)

lda2


library(MASS)
library(tidyverse)
library(Hotelling)

loans.test<-hotelling.test(loan_data$person_age+
    loan_data$person_income + loan_data$person_emp_exp+loan_data$loan_amnt +
    loan_data$loan_int_rate+ loan_data$loan_percent_income+loan_data$cb_person_cred_hist_length+
    loan_data$credit_score~factor(loan_data$loan_status),data=bankr )

loans.test

#LDA-Predict groups-taking a predictive view

lda3<-lda(loan_data$loan_status~loan_data$person_age+
            loan_data$person_income + loan_data$person_emp_exp+loan_data$loan_amnt +
            loan_data$loan_int_rate+ loan_data$loan_percent_income+loan_data$cb_person_cred_hist_length+
            loan_data$credit_score,prior=c(0.5,0.5),data=bankr )
lda3.results<-bankr%>%
  mutate(classify.loan_data.loan_status=predict(lda3)$class)

lda3.results

lda3$scaling

zbar1 <- sum(lda3$scaling*lda3$means[1,])
zbar1

zbar2<- sum(lda3$scaling*lda3$means[2,])

zbar2

cutoff<-0.5*(zbar1+zbar2)

cutoff

#checking classification results
with(lda3.results,table(loan_data.loan_status,classify.loan_data.loan_status))


#Classification Analysis Result

# overall classification  (13,508 + 14793)/45,000 : 63% Correct classification

# No default :13508/(13,508+8,634)

# Previous default : 14,793/(14,793+8,065)

pairs(bankr[,1:8])


```


Adding David's LDA results to table
```{R}

addresults('LDA', confusionMatrix(factor( lda3.results$classify.loan_data.loan_status), factor(lda3.results$loan_data.loan_status), positive = '1'))
rownames(results)<-NULL

# rejoining to get back the previous default data
temp <- lda3.results %>% left_join(data %>% select(person_age, person_income, loan_percent_income, person_emp_exp,credit_score,loan_int_rate,previous_loan_defaults_on_file), by=c("loan_data.person_age"="person_age", "loan_data.person_income"="person_income", "loan_data.loan_percent_income"="loan_percent_income", "loan_data.person_emp_exp" = "person_emp_exp", "loan_data.credit_score"="credit_score", "loan_data.loan_int_rate"= "loan_int_rate")) %>% mutate(classify.loan_data.loan_status=as.numeric(classify.loan_data.loan_status)-1)

addresults2('LDA', confusionMatrix(factor( ifelse(temp$previous_loan_defaults_on_file=='Yes',0,temp$classify.loan_data.loan_status ) ), factor(temp$loan_data.loan_status), positive = '1'))
rownames(results2)<-NULL

cbind("Model"=results[,1],round(results[,2:4],3))
cbind("Model"=results2[,1],round(results2[,2:4],3))
```


Redoing Lasso removing percent income
```{r}


# Extracting outcome variable
class <- cleanData$loan_status

# Converting to a matrix, exclude the non-log-transformed, and also the prior loan default
matrixData <- model.matrix(class ~ ., data = cleanData %>% select(-c("loan_status", "person_income", "person_emp_exp", "previous_loan_defaults_on_file_Yes", "loan_percent_income") ))

# First column is all intercepts
matrixData<- matrixData[,-1]

# Uses cross validation, no need to train/test split. Alpha selects lasso (rather than ridge regression), but I think thats the default anyways
lasso <- cv.glmnet(matrixData, class, alpha = 1, family = "binomial")

# See coefficients, which variables we might drop
# Now that we switched to log income, variable is no longer being dropped. We will proceed with log_income
coef(lasso)


```


```



